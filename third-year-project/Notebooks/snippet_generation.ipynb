{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da3c1e-71c4-484c-8e29-07b30fa54686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e243a-61c8-4632-87b9-18386188fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "platforms = [\"Pitchfork\", \"Guardian\", \"NME\", \"Spectrum\"]\n",
    "dfs = {platform: pd.read_hdf(\n",
    "    \"C:\\\\Users\\\\tommy\\\\OneDrive\\\\University\\\\Year 3\\\\Third Year Project\\\\Platform Album Data\\\\text_edited_data.h5\", key=platform).drop(\n",
    "    [\"Url\"], axis=1) for platform in platforms}\n",
    "\n",
    "text = {platform: df.loc[:, \"Text\"] for platform, df in dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5fd27-fe7a-4670-9cd7-ea42e486a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.nist import NISTTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nist_tokenizer = NISTTokenizer()\n",
    "data = dfs[\"Pitchfork\"].loc[0, \"Text\"]\n",
    "test_text = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c662bdc-ce64-43be-970f-80f9a43cd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_both(x):\n",
    "    \n",
    "    return [nist_tokenizer.tokenize(sentence) for sentence in sent_tokenize(x)]\n",
    "\n",
    "def replace_quotes(x):\n",
    "    \n",
    "    for match in re.findall(\" “.+?[”'\\1{2}]\", x):\n",
    "        x = x.replace(match, \"\")\n",
    "                \n",
    "    return x\n",
    "\n",
    "def preprocess_unigram(x):\n",
    "    \n",
    "    x = replace_quotes(x)\n",
    "    x = tokenize_both(x)\n",
    "    x = [[word for word in sentence if word[0] not in string.punctuation] for sentence in x]\n",
    "    x = [nltk.pos_tag(text) for text in x]\n",
    "    x = np.array([np.array([word for word in sentence if word[0].lower() not in stopwords.words(\"english\")]) for sentence in x])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def preprocess_bigram(x):\n",
    "    \n",
    "    convert_tuple = [[list(word) for word in sentence] for sentence in preprocess_unigram(x)]\n",
    "    \n",
    "    return [list(nltk.bigrams(sentence)) for sentence in convert_tuple]\n",
    "\n",
    "test_text = preprocess(test_text)\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4548ff2-4ebd-46bf-a656-168277ad3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "ops_count = np.array([sum(list(map(lambda x: x[0].lower() in opinion_lexicon.words(), sentence))) / len(sentence) for sentence in test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f7752-7861-45a6-a112-661007cce577",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e2ec2-0500-414d-95bd-10823a786614",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text[ops_count <= 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ea0f9-cc68-4ff2-a98e-494ee3695f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_count = np.array([sum(list(map(lambda x: x[1] in [\"NNP\", \"NN\"], sentence))) / len(sentence) for sentence in test_text])\n",
    "noun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644ed50-665d-466e-ab69-f30bf68419aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [list(nltk.bigrams(sentence)) for sentence in \n",
    "           [[list(word) for word in sentence] for sentence in preprocess(data)]]\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7bece-ac82-4241-a43a-3175947d7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_count = np.array([sum(list(map(lambda x: x[0][1] in [\"NNP\", \"NN\"] and x[1][1] in [\"NNP\", \"NN\"], sentence))) for sentence in bigrams])\n",
    "an_count = np.array([sum(list(map(lambda x: x[0][1] == \"JJ\" and x[1][1] in [\"NNP\", \"NN\"], sentence))) for sentence in bigrams])\n",
    "\n",
    "print(nn_count, an_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
